# -*- coding: utf-8 -*-
"""Neural-Networks-Code-Scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Cs9HQSX-xqTkfNmUAXkGfR5QGUuN7QT
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

torch.manual_seed(42)

df

df = pd.read_csv('/content/fmnist_small.csv')
df.head()

# Create a 4x4 grid of images
fig, axes = plt.subplots(4, 4, figsize=(10, 10))
fig.suptitle("First 16 Images", fontsize=16)

# Plot the first 16 images from the dataset
for i, ax in enumerate(axes.flat):
    img = df.iloc[i, 1:].values.reshape(28, 28)  # Reshape to 28x28
    ax.imshow(img)  # Display in grayscale
    ax.axis('off')  # Remove axis for a cleaner look
    ax.set_title(f"Label: {df.iloc[i, 0]}")  # Show the label

plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the title
plt.show()

X =  df.iloc[:,1:].values
y = df.iloc[:,0].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train / 255.0
y_train = y_train / 255.0

## create custom datasets
class CustomDataset(Dataset):
  def __init__(self, features , labels):
    self.features = torch.tensor(features , dtype = torch.float32)
    self.labels = torch.tensor(labels , dtype = torch.long)

  def __len__(self):
    return len(self.features)

  def __getitem__(self,index):
    return self.features[index], self.labels[index]

train_dataset = CustomDataset(X_train , y_train)

test_dataset = CustomDataset(X_test, y_test)

## train and test data loader

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
## train and test data loader

test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

class MyNN(nn.Module):
  def __init__(self , num_features):
    super().__init__()
    self.model = nn.Sequential(
        nn.Linear(num_features,128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64 ,10)


    )
  def forward(self, x):
    return self.model(x)

epochs = 100
learning_rate = 0.1

model = MyNN(X_train.shape[1])

criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)

## training loop
for epoch in range(epochs):
  total_epoch_loss = 0

  for batch_features , batch_labels in train_loader:
    ## forward pass
    outputs = model(batch_features)
    ## calculate the loss
    loss = criterion(outputs , batch_labels)
    ## backward pass
    optimizer.zero_grad()
    loss.backward()

    ## update the pass
    optimizer.step()


    total_epoch_loss = total_epoch_loss + loss.item()

  avg_loss = total_epoch_loss/len(train_loader)
  print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')

## model evaluation code
model.eval()

total = 0
correct = 0
with torch.no_grad():
  for batch_features , batch_labels in test_loader:
    outputs = model(batch_features)

    _, predicted = torch.max(outputs , 1)
    total = total + batch_labels.shape[0]
    correct = correct + (predicted == batch_labels).sum().item()
print(correct/total)

"""mnist-handwritten"""

from torchvision.datasets import MNIST
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])

train_dataset = MNIST(
    root="./data",
    train=True,
    download=True,
    transform=transform
)

test_dataset = MNIST(
    root="./data",
    train=False,
    download=True,
    transform=transform
)

import matplotlib.pyplot as plt

train_dataset

test_dataset

train_loader = DataLoader(train_dataset , batch_size = 32 , shuffle = True)

test_loader = DataLoader(test_dataset, batch_size=32,shuffle = False)

train_loader

class MyNN(nn.Module):
  def __init__(self,num_features):
    super().__init__()
    self.model = nn.Sequential(
        nn.Linear(num_features, 128),
        nn.ReLU(),
        nn.Linear(128 , 64),
        nn.ReLU(),
        nn.Linear(64,10),
       )

  def forward(self,x):
    return  self.model(x)

epochs = 100
learning_rate = 0.1

model = MyNN(num_features=784)

criterion = nn.CrossEntropyLoss()

import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(epochs):

    total_loss = 0

    for batch_features, batch_labels in train_loader:

        outputs = model(batch_features)          # 1️⃣ predict
        loss = criterion(outputs, batch_labels)  # 2️⃣ measure error

        optimizer.zero_grad()                     # 3️⃣ clear old grads
        loss.backward()                           # 4️⃣ compute grads
        optimizer.step()                          # 5️⃣ update weights

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}")

model.eval()

model.eval()          # switch to evaluation mode
correct = 0
total = 0

with torch.no_grad(): # no gradients needed
    for batch_features, batch_labels in test_loader:

        outputs = model(batch_features)
        _, predicted = torch.max(outputs, 1)

        total += batch_labels.size(0)
        correct += (predicted == batch_labels).sum().item()

accuracy = correct / total
print("Test Accuracy:", accuracy)

